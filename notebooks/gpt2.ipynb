{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:46:42.286272Z",
     "start_time": "2024-03-14T16:46:42.279667Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, GPT2Model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-1M')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "# prompt = \"Once upon a time there was\"\n",
    "\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:46:37.762001Z",
     "start_time": "2024-03-14T16:46:23.369985Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model_id = 'google-bert/bert-large-uncased-whole-word-masking'\n",
    "model_id = 'bert-base-cased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "model = BertModel.from_pretrained(model_id)\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)\n",
    "# print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T11:15:11.120999400Z",
     "start_time": "2024-02-23T11:15:08.282559900Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "generated = tokenizer.decode(output[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T22:37:03.436253300Z",
     "start_time": "2024-02-22T22:37:03.016566400Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "\n",
    "# model_id = 'google/gemma-7b'\n",
    "# model_id = 'google/gemma-7b-it'\n",
    "#   \n",
    "# gemma\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T11:23:35.406539300Z",
     "start_time": "2024-02-23T11:22:40.291533600Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")#.to(\"cuda\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "with torch.no_grad():\n",
    "# Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "    _ = model.generate(**input_ids, streamer=streamer, max_new_tokens=100)\n",
    "\n",
    "# print(tokenizer.decode(outputs[0]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:46:45.415088Z",
     "start_time": "2024-03-14T16:46:44.441594Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\")\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "##\n",
    "\n",
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "###\n",
    "\n",
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "# pip install bitsandbytes accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", quantization_config=quantization_config)\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "###\n",
    "\n",
    "# pip install bitsandbytes accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", quantization_config=quantization_config)\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"Problem 1: Find the derivative of the function f(x) = x^2 at the point x = 3. \\n\\nSolution:\"\n",
    "prompt = '''We are interested in building a causal model based on the explicit and implicit assumptions contained in the specified article and then using causal inference to evaluate the reasoning. Answer the following questions to design an interesting, simple, and most importantly realistic causal model from a news article.\n",
    "\n",
    "To help understand the instructions here are some tips:\n",
    "- all variables are always binary and (at least in principle) measurable, so when creating and selecting variables, make sure it is reasonable to treat them as binary\n",
    "- whenever you propose a variable, make sure to define the meaning of each value it can take, and mention whether it is observable or not\n",
    "- outcome variables are always observable, and should always have 2-3 causal parents (including treatment, mediator, and confounder variables)\n",
    "- treatment variables are always observable and intervenable, that means it must be possible to (at least in principle) change their value if desired, and these should generally have at least causal parent, and always at least one child\n",
    "- confounder variables may or may not be observable, and should always have 2-3 causal children and no causal parents\n",
    "- mediator variables may or may not be observable, and should always have 1-2 causal parents (for example, a treatment variable or confounder) and 1-2 causal children (for example, the outcome variable)\n",
    "- collider variables are always observable, and should always have 2-3 causal parents (for example, a treatment and outcome variable) and no causal children\n",
    "\n",
    "Here is the topic and original the news article headline:\n",
    "\n",
    "```\n",
    "Title: CFC ban halted climate catastrophe, scientists say - BBC News\n",
    "Description: A global treaty banning ozone-depleting chemicals in 1987 may have averted a climate catastrophe today.\n",
    "Original Language: English\n",
    "\n",
    "This article brings to light how the 1987 global treaty to ban CFCs, chemicals harming the ozone layer, might have helped us avoid severe climate issues today. To explore this further, we could look at historical temperature records, greenhouse gas levels, and ozone layer measurements over the past few decades. By comparing these data before and after the ban, we might see how significantly this action contributed to slowing down climate change.\n",
    "```\n",
    "\n",
    "Begin by brainstorming some non-trivial interesting binary causal variables to construct a causal bayes net:\n",
    "\n",
    "1. Propose 1 outcome variable mentioned implicitly or explicitly in the introduction addressing some quantity that people are most likely to be interested in studying, especially if people tend to have misconceptions about it\n",
    "3. Propose 2 treatment variables that either directly or indirectly affect the selected outcome variable and are the most interesting to study. Make sure the treatment variables affect the outcome (possibly through a mediator).\n",
    "4. Propose 2 confounder variables that affect some reasonable combination of the outcome, treatment, and mediator variables in a non-trivial way. Make sure the confounders have at least one child.\n",
    "5. Propose 2 mediator variables that affect and are affected by some reasonable combination of any other variables in a non-trivial way. Make sure the mediators have both parents and children.\n",
    "6. Propose 1 collider variable that are affected by some reasonable combination of any other variables in a non-trivial way. Make sure the collider has at least two parents.\n",
    "\n",
    "The variables and causal graph should, where possible, use specific details such as names and locations mentioned in the article. Also, generally the variable value \"0\" should correspond to the negative, neutral, or control while the value \"1\" should correspond to the positive choice or active value.\n",
    "\n",
    "As part of the brainstorming you should briefly first list the names of all the proposed variables.\n",
    "\n",
    "Next, construct a causal graph using the proposed variables by listing all the edges in the graph. Make sure to into the outcome variable and both treatment variables, and some interesting and intuitive combination of the other variables (you don't have to use all the others). Important: Make sure the causal graph is a DAG and that each node has at most THREE parents!\n",
    "\n",
    "Formalize this causal graph in the form of a JSON list of variables where each variable contains the following fields:\n",
    "- `name`: the name of the variable\n",
    "- `description`: a short description of the variable\n",
    "- `type`: the type of the variable, which can be one of the following: `outcome`, `treatment`, `confounder`, `mediator`, `collider`\n",
    "- `observed`: a boolean value indicating whether the variable is observable or not\n",
    "- `values`: a JSON list of the descriptions of the values the variable can take (corresponding to the index)\n",
    "- `parents`: a JSON list of the names of the parents of the variable (make sure they match the corresponding `name` field of the parent nodes, and remember, there should not be more than three parents for any node)\n",
    "\n",
    "Aside from some brief brainstorming, answer concisely and precisely with a JSON list in the desired format.\n",
    "\n",
    "To make sure you answer in the correct format with relevant information, here's an example of a causal graph that was generated based on a different article:\n",
    "\n",
    "```json\n",
    "[{'name': 'Global Economic Health',\n",
    "  'description': 'Overall health of the global economy',\n",
    "  'type': 'confounder',\n",
    "  'observed': True,\n",
    "  'values': ['Poor economic health', 'Good economic health'],\n",
    "  'parents': []},\n",
    " {'name': 'COVID-19 Pandemic Impact',\n",
    "  'description': 'Impact of the COVID-19 pandemic on the economy',\n",
    "  'type': 'treatment',\n",
    "  'observed': True,\n",
    "  'values': ['No impact', 'Impact'],\n",
    "  'parents': ['Global Economic Health']},\n",
    " {'name': 'Government Economic Policy',\n",
    "  'description': 'Government policies affecting the economy during the pandemic',\n",
    "  'type': 'treatment',\n",
    "  'observed': True,\n",
    "  'values': ['No policy change', 'Policy change'],\n",
    "  'parents': ['Global Economic Health']},\n",
    " {'name': 'Auto Industry Health',\n",
    "  'description': 'Financial health and stability of the auto industry',\n",
    "  'type': 'confounder',\n",
    "  'observed': True,\n",
    "  'values': ['Unstable', 'Stable'],\n",
    "  'parents': []},\n",
    " {'name': 'Company Financial Stability',\n",
    "  'description': 'Financial stability of individual companies in the auto industry',\n",
    "  'type': 'mediator',\n",
    "  'observed': True,\n",
    "  'values': ['Unstable', 'Stable'],\n",
    "  'parents': ['COVID-19 Pandemic Impact',\n",
    "   'Government Economic Policy',\n",
    "   'Auto Industry Health']},\n",
    " {'name': 'Public Demand for Cars',\n",
    "  'description': 'Consumer demand for automobiles',\n",
    "  'type': 'mediator',\n",
    "  'observed': True,\n",
    "  'values': ['Low demand', 'High demand'],\n",
    "  'parents': ['Auto Industry Health']},\n",
    " {'name': 'Job Loss in Auto Industry',\n",
    "  'description': 'Reduction in the number of jobs in the auto industry',\n",
    "  'type': 'outcome',\n",
    "  'observed': True,\n",
    "  'values': ['No job loss', 'Job loss'],\n",
    "  'parents': ['Company Financial Stability',\n",
    "   'Public Demand for Cars',\n",
    "   'COVID-19 Pandemic Impact']},\n",
    " {'name': 'Media Coverage of Job Losses',\n",
    "  'description': 'Extent of media coverage on job losses in the auto industry',\n",
    "  'type': 'collider',\n",
    "  'observed': True,\n",
    "  'values': ['Limited coverage', 'Extensive coverage'],\n",
    "  'parents': ['COVID-19 Pandemic Impact', 'Job Loss in Auto Industry']}]\n",
    "```\n",
    "\n",
    "Don't use the variables or edges in the above example for your answer, but use the format to answer the question.\n",
    "\n",
    "Take a deep breath and think step-by-step how you are going to do this.'''\n",
    "chat = [{ \"role\": \"user\", \"content\": prompt},]\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "with torch.no_grad():\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=1000, do_sample=True, temperature=1, top_k=50, top_p=0.95)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T13:25:24.777001278Z",
     "start_time": "2024-03-07T13:23:49.221434832Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pynvml"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:43:20.278304967Z",
     "start_time": "2024-02-16T16:43:20.234775457Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pynvml.smi import nvidia_smi\n",
    "nvsmi = nvidia_smi.getInstance()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:44:42.035801356Z",
     "start_time": "2024-02-16T16:44:42.012092606Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "nvidia_smi.getInstance().DeviceQuery('memory.free, memory.total')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T16:44:54.263365085Z",
     "start_time": "2024-02-16T16:44:54.256192541Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "device = 'cpu'\n",
    "\n",
    "# model_id = 'Mistral-7B-v0.1'\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "model_id = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "model_id = 'mistralai/Mixtral-8x7B-v0.1'\n",
    "# model_id = 'microsoft/phi-2'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T12:48:29.654179141Z",
     "start_time": "2024-02-15T12:43:35.557012793Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T17:19:27.150426009Z",
     "start_time": "2024-02-21T17:19:00.545359198Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)#, num_return_sequences: int = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T17:20:49.769811776Z",
     "start_time": "2024-02-21T17:20:49.563339369Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "greedy_output"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "outputs = model(**inputs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import pipeline\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "GPT2 = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "text_generation = pipeline('text-generation', model=GPT2, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "prefix_text = \"Question: What date did World War 1 end (answer with year-month-day)?\\nAnswer:\""
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# generated_text= text_generation(prefix_text, max_length=50, do_sample=False)[0]\n",
    "generated_text= text_generation(prefix_text, max_length=50, #do_sample=False,\n",
    "    num_beams = 5,\n",
    "    no_repeat_ngram_size = 2,\n",
    "    num_return_sequences = 5,\n",
    "    early_stopping = True)[0]\n",
    "\n",
    "print(generated_text['generated_text'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "text_generation.model"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import psutil\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T16:03:06.664665466Z",
     "start_time": "2024-02-15T16:03:05.800652080Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "psutil.cpu_percent(interval=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T16:00:52.572770775Z",
     "start_time": "2024-02-15T16:00:51.561731931Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "i = 0\n",
    "{\n",
    "        \"name\": torch.cuda.get_device_name(i),\n",
    "        \"total_memory_GB\": torch.cuda.get_device_properties(i).total_memory / (1024 ** 3),\n",
    "        \"memory_allocated_GB\": torch.cuda.memory_allocated(i) / 1e9,\n",
    "        \"memory_cached_GB\": torch.cuda.memory_reserved(i) / 1e9,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T16:11:17.590596158Z",
     "start_time": "2024-02-15T16:11:17.547618104Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "ram_info = psutil.virtual_memory()\n",
    "{\n",
    "    \"total_GB\": ram_info.total / (1024 ** 3),\n",
    "    \"available_GB\": ram_info.available / (1024 ** 3),\n",
    "    \"used_GB\": ram_info.used / (1024 ** 3),\n",
    "    \"percentage_used\": ram_info.percent\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T16:12:05.231528306Z",
     "start_time": "2024-02-15T16:12:05.187875696Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
