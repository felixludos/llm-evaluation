{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T14:52:20.750950Z",
     "start_time": "2025-03-03T14:52:20.746026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import csv, json, re\n",
    "import omnifig as fig\n",
    "from meval.datasets import SingleMMLU\n",
    "from omnibelt import colorize\n",
    "fig.initialize()"
   ],
   "id": "371011eebcbc1c08",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T14:51:14.758589Z",
     "start_time": "2025-03-03T14:51:14.750543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = Path('../../clones/mmlu/data/conceptual_physics_dev.csv')\n",
    "path.exists()"
   ],
   "id": "95f1cf44ce0ec983",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T14:51:14.840218Z",
     "start_time": "2025-03-03T14:51:14.828033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = SingleMMLU(path=path)\n",
    "dataset.load()"
   ],
   "id": "f36a3fc251a4b9a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleMMLU[5](question, choices, answer)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T14:52:35.426306Z",
     "start_time": "2025-03-03T14:52:35.420209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ctx = dataset.sample()\n",
    "print(ctx['question'])\n",
    "print('\\n'.join(colorize(c, 'green' if i == ctx['answer'] else 'red') for i, c in enumerate(ctx['choices'])))"
   ],
   "id": "b4c3509c51b32aed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Things that are equivalent according to the equivalence principle are\n",
      "\u001B[91mspace and time.\u001B[0m\n",
      "\u001B[91ma traveling twin and a stay-at-home twin.\u001B[0m\n",
      "\u001B[92mgravity and acceleration.\u001B[0m\n",
      "\u001B[91mmass and energy.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e73d7fb9eb4bbbbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T11:42:16.606840Z",
     "start_time": "2025-03-03T11:42:16.599931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = list(csv.DictReader(path.open('r')))\n",
    "len(data)"
   ],
   "id": "a2b6ea7017ca751d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T11:42:20.950478Z",
     "start_time": "2025-03-03T11:42:20.943298Z"
    }
   },
   "cell_type": "code",
   "source": "data",
   "id": "7503d51a990d9b67",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Compared with the mass of a uranium atom undergoing fission, the combined masses of the products after fission are': 'Things that are equivalent according to the equivalence principle are',\n",
       "  'less': 'space and time.',\n",
       "  'more': 'a traveling twin and a stay-at-home twin.',\n",
       "  'the same': 'gravity and acceleration.',\n",
       "  'zero': 'mass and energy.',\n",
       "  'A': 'C'},\n",
       " {'Compared with the mass of a uranium atom undergoing fission, the combined masses of the products after fission are': 'Colors in a soap bubble result from light',\n",
       "  'less': 'converted to a different frequency',\n",
       "  'more': 'deflection',\n",
       "  'the same': 'interference',\n",
       "  'zero': 'polarization',\n",
       "  'A': 'C'},\n",
       " {'Compared with the mass of a uranium atom undergoing fission, the combined masses of the products after fission are': 'A model airplane flies slower when flying into the wind and faster with wind at its back. When launched at right angles to the wind a cross wind its groundspeed compared with flying in still air is',\n",
       "  'less': 'the same',\n",
       "  'more': 'greater',\n",
       "  'the same': 'less',\n",
       "  'zero': 'either greater or less depending on wind speed',\n",
       "  'A': 'B'},\n",
       " {'Compared with the mass of a uranium atom undergoing fission, the combined masses of the products after fission are': 'Which of these three elements has the most mass per nucleon?',\n",
       "  'less': 'Hydrogen',\n",
       "  'more': 'Iron',\n",
       "  'the same': 'Uranium',\n",
       "  'zero': 'Same in each',\n",
       "  'A': 'A'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f2b4504d9048bac4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T22:31:09.390566Z",
     "start_time": "2025-03-01T22:31:09.366490Z"
    }
   },
   "cell_type": "code",
   "source": "from RestrictedPython import compile_restricted",
   "id": "7f27d4cb16988b2f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T22:29:26.199319Z",
     "start_time": "2025-03-01T22:29:26.189597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(\n",
    "\t[1, 'hello'],\n",
    "\tTrue\n",
    ")"
   ],
   "id": "290bb091f78d3927",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 'hello'], True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T22:29:37.344208Z",
     "start_time": "2025-03-01T22:29:37.339208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dat = '''(\n",
    "\t[1, 'hello'],\n",
    "\tTrue\n",
    ")'''"
   ],
   "id": "501b2207937d40f3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T22:32:24.190394Z",
     "start_time": "2025-03-01T22:32:24.181934Z"
    }
   },
   "cell_type": "code",
   "source": "eval(compile_restricted(dat, \"<string>\", \"eval\"))",
   "id": "f3b1bacdc519a3aa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 'hello'], True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "87870695f8f1fddf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T22:33:35.474882Z",
     "start_time": "2025-03-01T22:33:35.468883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "safe_code = \"\"\"\n",
    "def safe_function():\n",
    "    return \"Hello from sandbox!\"\n",
    "result2 = safe_function()\n",
    "\"\"\"\n",
    "\n",
    "# compiled_code = compile_restricted(safe_code, \"<string>\", \"exec\")\n",
    "# exec(compiled_code)"
   ],
   "id": "b418fe8cd643697",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T22:33:41.724528Z",
     "start_time": "2025-03-01T22:33:41.718860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "compile_restricted(safe_code, \"<string>\", \"exec\")\n",
    "result2"
   ],
   "id": "8cbaee967cf7e8c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello from sandbox!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ad0e15b3bea89709"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T15:25:39.345119Z",
     "start_time": "2025-03-05T15:25:39.286739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subcategories = {\n",
    "    \"abstract_algebra\": [\"math\"],\n",
    "    \"anatomy\": [\"health\"],\n",
    "    \"astronomy\": [\"physics\"],\n",
    "    \"business_ethics\": [\"business\"],\n",
    "    \"clinical_knowledge\": [\"health\"],\n",
    "    \"college_biology\": [\"biology\"],\n",
    "    \"college_chemistry\": [\"chemistry\"],\n",
    "    \"college_computer_science\": [\"computer science\"],\n",
    "    \"college_mathematics\": [\"math\"],\n",
    "    \"college_medicine\": [\"health\"],\n",
    "    \"college_physics\": [\"physics\"],\n",
    "    \"computer_security\": [\"computer science\"],\n",
    "    \"conceptual_physics\": [\"physics\"],\n",
    "    \"econometrics\": [\"economics\"],\n",
    "    \"electrical_engineering\": [\"engineering\"],\n",
    "    \"elementary_mathematics\": [\"math\"],\n",
    "    \"formal_logic\": [\"philosophy\"],\n",
    "    \"global_facts\": [\"other\"],\n",
    "    \"high_school_biology\": [\"biology\"],\n",
    "    \"high_school_chemistry\": [\"chemistry\"],\n",
    "    \"high_school_computer_science\": [\"computer science\"],\n",
    "    \"high_school_european_history\": [\"history\"],\n",
    "    \"high_school_geography\": [\"geography\"],\n",
    "    \"high_school_government_and_politics\": [\"politics\"],\n",
    "    \"high_school_macroeconomics\": [\"economics\"],\n",
    "    \"high_school_mathematics\": [\"math\"],\n",
    "    \"high_school_microeconomics\": [\"economics\"],\n",
    "    \"high_school_physics\": [\"physics\"],\n",
    "    \"high_school_psychology\": [\"psychology\"],\n",
    "    \"high_school_statistics\": [\"math\"],\n",
    "    \"high_school_us_history\": [\"history\"],\n",
    "    \"high_school_world_history\": [\"history\"],\n",
    "    \"human_aging\": [\"health\"],\n",
    "    \"human_sexuality\": [\"culture\"],\n",
    "    \"international_law\": [\"law\"],\n",
    "    \"jurisprudence\": [\"law\"],\n",
    "    \"logical_fallacies\": [\"philosophy\"],\n",
    "    \"machine_learning\": [\"computer science\"],\n",
    "    \"management\": [\"business\"],\n",
    "    \"marketing\": [\"business\"],\n",
    "    \"medical_genetics\": [\"health\"],\n",
    "    \"miscellaneous\": [\"other\"],\n",
    "    \"moral_disputes\": [\"philosophy\"],\n",
    "    \"moral_scenarios\": [\"philosophy\"],\n",
    "    \"nutrition\": [\"health\"],\n",
    "    \"philosophy\": [\"philosophy\"],\n",
    "    \"prehistory\": [\"history\"],\n",
    "    \"professional_accounting\": [\"other\"],\n",
    "    \"professional_law\": [\"law\"],\n",
    "    \"professional_medicine\": [\"health\"],\n",
    "    \"professional_psychology\": [\"psychology\"],\n",
    "    \"public_relations\": [\"politics\"],\n",
    "    \"security_studies\": [\"politics\"],\n",
    "    \"sociology\": [\"culture\"],\n",
    "    \"us_foreign_policy\": [\"politics\"],\n",
    "    \"virology\": [\"health\"],\n",
    "    \"world_religions\": [\"philosophy\"],\n",
    "}\n",
    "\n",
    "categories = {\n",
    "    \"STEM\": [\"physics\", \"chemistry\", \"biology\", \"computer science\", \"math\", \"engineering\"],\n",
    "    \"humanities\": [\"history\", \"philosophy\", \"law\"],\n",
    "    \"social sciences\": [\"politics\", \"culture\", \"economics\", \"geography\", \"psychology\"],\n",
    "    \"other (business, health, misc.)\": [\"other\", \"business\", \"health\"],\n",
    "}\n",
    "\n",
    "subjects = {}\n"
   ],
   "id": "c977d22c68a168e1",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T23:05:04.978172Z",
     "start_time": "2025-03-05T23:05:04.971975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_mmlu_subjects = {}\n",
    "for sub, cats in subcategories.items():\n",
    "\tfor cat in cats:\n",
    "\t\t_mmlu_subjects.setdefault(cat, []).append(sub)\n"
   ],
   "id": "894d009e0e283a2c",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T23:05:05.197636Z",
     "start_time": "2025-03-05T23:05:05.189574Z"
    }
   },
   "cell_type": "code",
   "source": "_mmlu_subjects",
   "id": "f41d20a6b2b1350e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'math': ['abstract_algebra',\n",
       "  'college_mathematics',\n",
       "  'elementary_mathematics',\n",
       "  'high_school_mathematics',\n",
       "  'high_school_statistics'],\n",
       " 'health': ['anatomy',\n",
       "  'clinical_knowledge',\n",
       "  'college_medicine',\n",
       "  'human_aging',\n",
       "  'medical_genetics',\n",
       "  'nutrition',\n",
       "  'professional_medicine',\n",
       "  'virology'],\n",
       " 'physics': ['astronomy',\n",
       "  'college_physics',\n",
       "  'conceptual_physics',\n",
       "  'high_school_physics'],\n",
       " 'business': ['business_ethics', 'management', 'marketing'],\n",
       " 'biology': ['college_biology', 'high_school_biology'],\n",
       " 'chemistry': ['college_chemistry', 'high_school_chemistry'],\n",
       " 'computer science': ['college_computer_science',\n",
       "  'computer_security',\n",
       "  'high_school_computer_science',\n",
       "  'machine_learning'],\n",
       " 'economics': ['econometrics',\n",
       "  'high_school_macroeconomics',\n",
       "  'high_school_microeconomics'],\n",
       " 'engineering': ['electrical_engineering'],\n",
       " 'philosophy': ['formal_logic',\n",
       "  'logical_fallacies',\n",
       "  'moral_disputes',\n",
       "  'moral_scenarios',\n",
       "  'philosophy',\n",
       "  'world_religions'],\n",
       " 'other': ['global_facts', 'miscellaneous', 'professional_accounting'],\n",
       " 'history': ['high_school_european_history',\n",
       "  'high_school_us_history',\n",
       "  'high_school_world_history',\n",
       "  'prehistory'],\n",
       " 'geography': ['high_school_geography'],\n",
       " 'politics': ['high_school_government_and_politics',\n",
       "  'public_relations',\n",
       "  'security_studies',\n",
       "  'us_foreign_policy'],\n",
       " 'psychology': ['high_school_psychology', 'professional_psychology'],\n",
       " 'culture': ['human_sexuality', 'sociology'],\n",
       " 'law': ['international_law', 'jurisprudence', 'professional_law']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T19:30:01.312206Z",
     "start_time": "2025-03-05T19:30:01.299999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hierarchy = {\n",
    "\t'full': ['STEM', 'humanities', 'social sciences', 'other (business, health, misc.)'],\n",
    "\t**categories,\n",
    "}\n",
    "for topic, subs in subcategories.items():\n",
    "\tfor s in subs:\n",
    "\t\thierarchy.setdefault(s, []).append(topic)\n",
    "for topic, subs in subcategories.items():\n",
    "\tif topic not in hierarchy:\n",
    "\t\thierarchy[topic] = []\n",
    "hierarchy"
   ],
   "id": "e60cac0bd338e385",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full': ['STEM',\n",
       "  'humanities',\n",
       "  'social sciences',\n",
       "  'other (business, health, misc.)'],\n",
       " 'STEM': ['physics',\n",
       "  'chemistry',\n",
       "  'biology',\n",
       "  'computer science',\n",
       "  'math',\n",
       "  'engineering'],\n",
       " 'humanities': ['history', 'philosophy', 'law'],\n",
       " 'social sciences': ['politics',\n",
       "  'culture',\n",
       "  'economics',\n",
       "  'geography',\n",
       "  'psychology'],\n",
       " 'other (business, health, misc.)': ['other', 'business', 'health'],\n",
       " 'math': ['abstract_algebra',\n",
       "  'college_mathematics',\n",
       "  'elementary_mathematics',\n",
       "  'high_school_mathematics',\n",
       "  'high_school_statistics'],\n",
       " 'health': ['anatomy',\n",
       "  'clinical_knowledge',\n",
       "  'college_medicine',\n",
       "  'human_aging',\n",
       "  'medical_genetics',\n",
       "  'nutrition',\n",
       "  'professional_medicine',\n",
       "  'virology'],\n",
       " 'physics': ['astronomy',\n",
       "  'college_physics',\n",
       "  'conceptual_physics',\n",
       "  'high_school_physics'],\n",
       " 'business': ['business_ethics', 'management', 'marketing'],\n",
       " 'biology': ['college_biology', 'high_school_biology'],\n",
       " 'chemistry': ['college_chemistry', 'high_school_chemistry'],\n",
       " 'computer science': ['college_computer_science',\n",
       "  'computer_security',\n",
       "  'high_school_computer_science',\n",
       "  'machine_learning'],\n",
       " 'economics': ['econometrics',\n",
       "  'high_school_macroeconomics',\n",
       "  'high_school_microeconomics'],\n",
       " 'engineering': ['electrical_engineering'],\n",
       " 'philosophy': ['formal_logic',\n",
       "  'logical_fallacies',\n",
       "  'moral_disputes',\n",
       "  'moral_scenarios',\n",
       "  'philosophy',\n",
       "  'world_religions'],\n",
       " 'other': ['global_facts', 'miscellaneous', 'professional_accounting'],\n",
       " 'history': ['high_school_european_history',\n",
       "  'high_school_us_history',\n",
       "  'high_school_world_history',\n",
       "  'prehistory'],\n",
       " 'geography': ['high_school_geography'],\n",
       " 'politics': ['high_school_government_and_politics',\n",
       "  'public_relations',\n",
       "  'security_studies',\n",
       "  'us_foreign_policy'],\n",
       " 'psychology': ['high_school_psychology', 'professional_psychology'],\n",
       " 'culture': ['human_sexuality', 'sociology'],\n",
       " 'law': ['international_law', 'jurisprudence', 'professional_law'],\n",
       " 'abstract_algebra': [],\n",
       " 'anatomy': [],\n",
       " 'astronomy': [],\n",
       " 'business_ethics': [],\n",
       " 'clinical_knowledge': [],\n",
       " 'college_biology': [],\n",
       " 'college_chemistry': [],\n",
       " 'college_computer_science': [],\n",
       " 'college_mathematics': [],\n",
       " 'college_medicine': [],\n",
       " 'college_physics': [],\n",
       " 'computer_security': [],\n",
       " 'conceptual_physics': [],\n",
       " 'econometrics': [],\n",
       " 'electrical_engineering': [],\n",
       " 'elementary_mathematics': [],\n",
       " 'formal_logic': [],\n",
       " 'global_facts': [],\n",
       " 'high_school_biology': [],\n",
       " 'high_school_chemistry': [],\n",
       " 'high_school_computer_science': [],\n",
       " 'high_school_european_history': [],\n",
       " 'high_school_geography': [],\n",
       " 'high_school_government_and_politics': [],\n",
       " 'high_school_macroeconomics': [],\n",
       " 'high_school_mathematics': [],\n",
       " 'high_school_microeconomics': [],\n",
       " 'high_school_physics': [],\n",
       " 'high_school_psychology': [],\n",
       " 'high_school_statistics': [],\n",
       " 'high_school_us_history': [],\n",
       " 'high_school_world_history': [],\n",
       " 'human_aging': [],\n",
       " 'human_sexuality': [],\n",
       " 'international_law': [],\n",
       " 'jurisprudence': [],\n",
       " 'logical_fallacies': [],\n",
       " 'machine_learning': [],\n",
       " 'management': [],\n",
       " 'marketing': [],\n",
       " 'medical_genetics': [],\n",
       " 'miscellaneous': [],\n",
       " 'moral_disputes': [],\n",
       " 'moral_scenarios': [],\n",
       " 'nutrition': [],\n",
       " 'prehistory': [],\n",
       " 'professional_accounting': [],\n",
       " 'professional_law': [],\n",
       " 'professional_medicine': [],\n",
       " 'professional_psychology': [],\n",
       " 'public_relations': [],\n",
       " 'security_studies': [],\n",
       " 'sociology': [],\n",
       " 'us_foreign_policy': [],\n",
       " 'virology': [],\n",
       " 'world_religions': []}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T19:37:48.616943Z",
     "start_time": "2025-03-05T19:37:48.602530Z"
    }
   },
   "cell_type": "code",
   "source": "print(hierarchy)",
   "id": "bbf96760ce255178",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full': ['STEM', 'humanities', 'social sciences', 'other (business, health, misc.)'], 'STEM': ['physics', 'chemistry', 'biology', 'computer science', 'math', 'engineering'], 'humanities': ['history', 'philosophy', 'law'], 'social sciences': ['politics', 'culture', 'economics', 'geography', 'psychology'], 'other (business, health, misc.)': ['other', 'business', 'health'], 'math': ['abstract_algebra', 'college_mathematics', 'elementary_mathematics', 'high_school_mathematics', 'high_school_statistics'], 'health': ['anatomy', 'clinical_knowledge', 'college_medicine', 'human_aging', 'medical_genetics', 'nutrition', 'professional_medicine', 'virology'], 'physics': ['astronomy', 'college_physics', 'conceptual_physics', 'high_school_physics'], 'business': ['business_ethics', 'management', 'marketing'], 'biology': ['college_biology', 'high_school_biology'], 'chemistry': ['college_chemistry', 'high_school_chemistry'], 'computer science': ['college_computer_science', 'computer_security', 'high_school_computer_science', 'machine_learning'], 'economics': ['econometrics', 'high_school_macroeconomics', 'high_school_microeconomics'], 'engineering': ['electrical_engineering'], 'philosophy': ['formal_logic', 'logical_fallacies', 'moral_disputes', 'moral_scenarios', 'philosophy', 'world_religions'], 'other': ['global_facts', 'miscellaneous', 'professional_accounting'], 'history': ['high_school_european_history', 'high_school_us_history', 'high_school_world_history', 'prehistory'], 'geography': ['high_school_geography'], 'politics': ['high_school_government_and_politics', 'public_relations', 'security_studies', 'us_foreign_policy'], 'psychology': ['high_school_psychology', 'professional_psychology'], 'culture': ['human_sexuality', 'sociology'], 'law': ['international_law', 'jurisprudence', 'professional_law'], 'abstract_algebra': [], 'anatomy': [], 'astronomy': [], 'business_ethics': [], 'clinical_knowledge': [], 'college_biology': [], 'college_chemistry': [], 'college_computer_science': [], 'college_mathematics': [], 'college_medicine': [], 'college_physics': [], 'computer_security': [], 'conceptual_physics': [], 'econometrics': [], 'electrical_engineering': [], 'elementary_mathematics': [], 'formal_logic': [], 'global_facts': [], 'high_school_biology': [], 'high_school_chemistry': [], 'high_school_computer_science': [], 'high_school_european_history': [], 'high_school_geography': [], 'high_school_government_and_politics': [], 'high_school_macroeconomics': [], 'high_school_mathematics': [], 'high_school_microeconomics': [], 'high_school_physics': [], 'high_school_psychology': [], 'high_school_statistics': [], 'high_school_us_history': [], 'high_school_world_history': [], 'human_aging': [], 'human_sexuality': [], 'international_law': [], 'jurisprudence': [], 'logical_fallacies': [], 'machine_learning': [], 'management': [], 'marketing': [], 'medical_genetics': [], 'miscellaneous': [], 'moral_disputes': [], 'moral_scenarios': [], 'nutrition': [], 'prehistory': [], 'professional_accounting': [], 'professional_law': [], 'professional_medicine': [], 'professional_psychology': [], 'public_relations': [], 'security_studies': [], 'sociology': [], 'us_foreign_policy': [], 'virology': [], 'world_religions': []}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "_subject_titles = {\n",
    "\t'full': 'MMLU', 'STEM': 'STEM', 'humanities': 'Humanities', 'social sciences': 'Social Sciences',\n",
    "\t'other (business, health, misc.)': 'Business/Health/Misc', 'math': 'Mathematics', 'physics': 'Physics',\n",
    "\t'chemistry': 'Chemistry', 'biology': 'Biology', 'computer science': 'Computer Science', 'engineering': 'Engineering',\n",
    "\t'history': 'History', 'philosophy': 'Philosophy', 'law': 'Law', 'politics': 'Politics', 'culture': 'Culture',\n",
    "\t'economics': 'Economics', 'geography': 'Geography', 'psychology': 'Psychology', 'other': 'Other', 'business': 'Business',\n",
    "\t'health': 'Health',\n",
    "\t'abstract_algebra': 'Abstract Algebra', 'anatomy': 'Anatomy', 'astronomy': 'Astronomy', 'business_ethics': 'Business Ethics',\n",
    "\t'clinical_knowledge': 'Clinical Knowledge', 'college_biology': 'College Biology', 'college_chemistry': 'College Chemistry',\n",
    "\t'college_computer_science': 'College Computer Science', 'college_mathematics': 'College Mathematics',\n",
    "\t'college_medicine': 'College Medicine', 'college_physics': 'College Physics', 'computer_security': 'Computer Security',\n",
    "\t'conceptual_physics': 'Conceptual Physics', 'econometrics': 'Econometrics', 'electrical_engineering': 'Electrical Engineering',\n",
    "\t'elementary_mathematics': 'Elementary Mathematics', 'formal_logic': 'Formal Logic', 'global_facts': 'Global Facts',\n",
    "\t'high_school_biology': 'High School Biology', 'high_school_chemistry': 'High School Chemistry',\n",
    "\t'high_school_computer_science': 'High School Computer Science', 'high_school_european_history': 'High School European History',\n",
    "\t'high_school_geography': 'High School Geography', 'high_school_government_and_politics': 'High School Government and Politics',\n",
    "\t'high_school_macroeconomics': 'High School Macroeconomics', 'high_school_mathematics': 'High School Mathematics',\n",
    "\t'high_school_microeconomics': 'High School Microeconomics', 'high_school_physics': 'High School Physics',\n",
    "\t'high_school_psychology': 'High School Psychology', 'high_school_statistics': 'High School Statistics',\n",
    "\t'high_school_us_history': 'High School US History', 'high_school_world_history': 'High School World History',\n",
    "\t'human_aging': 'Human Aging', 'human_sexuality': 'Human Sexuality', 'international_law': 'International Law',\n",
    "\t'jurisprudence': 'Jurisprudence', 'logical_fallacies': 'Logical Fallacies', 'machine_learning': 'Machine Learning',\n",
    "\t'management': 'Management', 'marketing': 'Marketing', 'medical_genetics': 'Medical Genetics', 'miscellaneous': 'Miscellaneous',\n",
    "\t'moral_disputes': 'Moral Disputes', 'moral_scenarios': 'Moral Scenarios', 'nutrition': 'Nutrition', 'prehistory': 'Prehistory',\n",
    "\t'professional_accounting': 'Professional Accounting', 'professional_law': 'Professional Law', 'professional_medicine': 'Professional Medicine',\n",
    "\t'professional_psychology': 'Professional Psychology', 'public_relations': 'Public Relations', 'security_studies': 'Security Studies',\n",
    "\t'sociology': 'Sociology', 'us_foreign_policy': 'US Foreign Policy', 'virology': 'Virology', 'world_religions': 'World Religions',\n",
    "}"
   ],
   "id": "d09b3ecf96e70c64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
