{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-15T12:54:45.158253567Z",
     "start_time": "2024-02-15T12:54:45.107835683Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2Model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "configuration_phi.py:   0%|          | 0.00/9.26k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4971acbb38a442b69c2962f038708070"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fleeb/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /is/cluster/fast/fleeb/huggingface_cache/hub/models--microsoft--phi-2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "  warnings.warn(message)\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- configuration_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": "modeling_phi.py:   0%|          | 0.00/62.7k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f94c8093718a4b87836cf9923d9918d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- modeling_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12be42aaad2d4a4db66d9501709c2191"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T12:53:01.360096515Z",
     "start_time": "2024-02-15T12:49:25.867743839Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using the power rule, we get:\n",
      "f'(x) = 2x\n",
      "f'(3) = 2(3) = 6\n",
      "Problem 2: Find the derivative of the function g(x) = sin(x) at the point x = pi/4. \n",
      "Solution: Using the chain rule, we get:\n",
      "g'(x) = cos(x)\n",
      "g'(pi/4) = cos(pi/\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Problem 1: Find the derivative of the function f(x) = x^2 at the point x = 3. \\n\\nSolution:\"\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "with torch.no_grad():\n",
    "# Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T12:55:35.748980698Z",
     "start_time": "2024-02-15T12:55:32.411467217Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/863 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60b6739305bd4536ac2df491c1e6e46e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fleeb/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /is/cluster/fast/fleeb/huggingface_cache/hub/models--microsoft--phi-2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8637ed91cbae409889d56fdf5c22d250"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb6a4a628df34a10ae7cfc6418af98bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa6eebe435c949c3ae0c147da1117dbc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03df1b267c2140ee9bab816ee34a0f26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f589f9b038b540e0be0838f324d8827e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/phi-2 were not used when initializing PhiForCausalLM: ['model.layers.14.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.9.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing PhiForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PhiForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['model.layers.30.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.21.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.30.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.bias', 'model.layers.24.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.10.self_attn.query_key_value.bias', 'model.layers.31.self_attn.query_key_value.weight', 'model.layers.26.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.weight', 'model.layers.26.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.weight', 'model.layers.25.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.31.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.16.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.weight', 'model.layers.17.self_attn.query_key_value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6709ea3b6ebc4b20874c6a2d39f5bc67"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3651f6bbcbf242fd8d1422569488bddb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a3ed70b4b34448a9b20dbadcd1c1459"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91cdfdc83b4e4cda9db55226aca8d511"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "946d5937d24e471e966ef323d5a1f01e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a528b099c3d848c1971ad9aaf8e26e79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0dad2fe795a649db826e545953faa986"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "device = 'cpu'\n",
    "\n",
    "# model_id = 'Mistral-7B-v0.1'\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "model_id = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "model_id = 'mistralai/Mixtral-8x7B-v0.1'\n",
    "# model_id = 'microsoft/phi-2'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T12:48:29.654179141Z",
     "start_time": "2024-02-15T12:43:35.557012793Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fleeb/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /is/cluster/fast/fleeb/huggingface_cache/hub/models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T12:14:51.041596950Z",
     "start_time": "2024-02-15T12:14:46.766910128Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-04dbe340301d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# generate text until the output length (which includes the context length) reaches 50\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mgreedy_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m50\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     26\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mF\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py\u001B[0m in \u001B[0;36mgenerate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m         \"\"\"\n\u001B[0;32m   1191\u001B[0m         \u001B[1;31m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1192\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_model_class\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1193\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1194\u001B[0m         \u001B[1;31m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py\u001B[0m in \u001B[0;36m_validate_model_class\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1083\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mgenerate_compatible_classes\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1084\u001B[0m                 \u001B[0mexception_message\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;34mf\" Please use one of the following classes instead: {generate_compatible_classes}\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1085\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mexception_message\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1086\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1087\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_validate_model_kwargs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_kwargs\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mDict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mAny\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "greedy_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2Model(\n  (wte): Embedding(50257, 768)\n  (wpe): Embedding(1024, 768)\n  (drop): Dropout(p=0.1, inplace=False)\n  (h): ModuleList(\n    (0): GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (1): GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (2): GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (3): GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (4): GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (5): GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (6): GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (7): GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (8): GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (9): GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (10): GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (11): GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "GPT2 = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "text_generation = pipeline('text-generation', model=GPT2, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "prefix_text = \"Question: What date did World War 1 end (answer with year-month-day)?\\nAnswer:\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What date did World War 1 end (answer with year-month-day)?\n",
      "Answer: The date of the end of WW1.\n",
      "Source: http://en.wikipedia.org/wiki/World_War_1_end\n"
     ]
    }
   ],
   "source": [
    "# generated_text= text_generation(prefix_text, max_length=50, do_sample=False)[0]\n",
    "generated_text= text_generation(prefix_text, max_length=50, #do_sample=False,\n",
    "    num_beams = 5,\n",
    "    no_repeat_ngram_size = 2,\n",
    "    num_return_sequences = 5,\n",
    "    early_stopping = True)[0]\n",
    "\n",
    "print(generated_text['generated_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation.model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import psutil\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T16:03:06.664665466Z",
     "start_time": "2024-02-15T16:03:05.800652080Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "24.7"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.cpu_percent(interval=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T16:00:52.572770775Z",
     "start_time": "2024-02-15T16:00:51.561731931Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'name': 'NVIDIA GeForce RTX 2070',\n 'total_memory_GB': 7.78466796875,\n 'memory_allocated_GB': 0.0,\n 'memory_cached_GB': 0.0}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "{\n",
    "        \"name\": torch.cuda.get_device_name(i),\n",
    "        \"total_memory_GB\": torch.cuda.get_device_properties(i).total_memory / (1024 ** 3),\n",
    "        \"memory_allocated_GB\": torch.cuda.memory_allocated(i) / 1e9,\n",
    "        \"memory_cached_GB\": torch.cuda.memory_reserved(i) / 1e9,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T16:11:17.590596158Z",
     "start_time": "2024-02-15T16:11:17.547618104Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'total_GB': 62.70671844482422,\n 'available_GB': 52.40065383911133,\n 'used_GB': 9.391868591308594,\n 'percentage_used': 16.4}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ram_info = psutil.virtual_memory()\n",
    "{\n",
    "    \"total_GB\": ram_info.total / (1024 ** 3),\n",
    "    \"available_GB\": ram_info.available / (1024 ** 3),\n",
    "    \"used_GB\": ram_info.used / (1024 ** 3),\n",
    "    \"percentage_used\": ram_info.percent\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T16:12:05.231528306Z",
     "start_time": "2024-02-15T16:12:05.187875696Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
